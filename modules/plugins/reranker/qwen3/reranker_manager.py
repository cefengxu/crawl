# from transformers import AutoTokenizer
# from vllm.engine.arg_utils import AsyncEngineArgs
# from vllm.engine.async_llm_engine import AsyncLLMEngine
# from vllm.sampling_params import SamplingParams
# from vllm.utils import random_uuid

# import os, math
# os.environ["TORCH_COMPILE_DISABLE"] = "1"
# os.environ["CUDA_VISIBLE_DEVICES"] = "0"  # æ˜ç¡®æŒ‡å®šä½¿ç”¨ç¬¬0å¡

# def format_instruction(instruction, query, doc):
#         return [
#             {"role": "system", "content": "Judge whether the Document meets the requirements based on the Query and the Instruct provided. Note that the answer can only be \"yes\" or \"no\"."},
#             {"role": "user", "content": f"<Instruct>: {instruction}\n\n<Query>: {query}\n\n<Document>: {doc}"}
#         ]

# class RerankerManager():
#     def __init__(self):
        
#         # é…ç½®å‚æ•°
#         MODEL_NAME = "/home/maosy6/Qwen3-Reranker-0.6B"
#         self.MAX_LENGTH = 8192
#         self.SUFFIX = "<|im_end|>\n<|im_start|>assistant\n<think>\n\n</think>\n\n"
        
#         # åˆå§‹åŒ–ç»„ä»¶
#         self.tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
#         self.tokenizer.padding_side = "left"
#         self.tokenizer.pad_token = self.tokenizer.eos_token
#         suffix_tokens = self.tokenizer.encode(self.SUFFIX, add_special_tokens=False)
#         self.true_token = self.tokenizer("yes", add_special_tokens=False).input_ids[0]
#         self.false_token = self.tokenizer("no", add_special_tokens=False).input_ids[0]
        
#         # åˆå§‹åŒ–vLLMå¼•æ“ï¼Œåªä½¿ç”¨ç¬¬0å¡ï¼Œä¼˜åŒ–æ˜¾å­˜ä½¿ç”¨
#         engine_args = AsyncEngineArgs(
#             model=MODEL_NAME,
#             tensor_parallel_size=1,
#             max_model_len=2048,  # å‡å°‘æœ€å¤§åºåˆ—é•¿åº¦
#             gpu_memory_utilization=0.3,  # å¤§å¹…å‡å°‘æ˜¾å­˜ä½¿ç”¨æ¯”ä¾‹
#             enable_prefix_caching=False,  # ç¦ç”¨å‰ç¼€ç¼“å­˜èŠ‚çœæ˜¾å­˜
#             max_num_batched_tokens=2048,  # é™åˆ¶æ‰¹å¤„ç†tokenæ•°
#             max_num_seqs=16,  # é™åˆ¶å¹¶å‘åºåˆ—æ•°
#         )
        
#         self.engine = AsyncLLMEngine.from_engine_args(engine_args)


#     def process_inputs(self, text_1, text_2, instruction):
#         prompts = []
#         for text1, text2 in zip(text_1, text_2):
#             messages = format_instruction(instruction, text1, text2)
#             prompt = self.tokenizer.apply_chat_template(
#                 messages, tokenize=False, add_generation_prompt=False
#             )
#             prompt = prompt[:self.MAX_LENGTH] + self.SUFFIX
#             prompts.append(prompt)
#         return prompts

#     async def rerank(self, text_1, text_2, instruction):
        
#         # å¤„ç†è¾“å…¥
#         prompts = self.process_inputs(text_1, text_2, instruction)        
        
#         # åˆ›å»ºé‡‡æ ·å‚æ•°
#         sampling_params = SamplingParams(
#             temperature=0,
#             max_tokens=1,
#             logprobs=20,
#             allowed_token_ids=[self.true_token, self.false_token]
#         )
        
#         # åˆ›å»ºè¯·æ±‚å¹¶è·å–ç»“æœ
#         outputs = []
#         request_ids = []
#         for prompt in prompts:
#             request_id = random_uuid()
#             request_ids.append(request_id)
#             outputs.append(
#                 self.engine.generate(
#                     prompt=prompt,
#                     sampling_params=sampling_params,
#                     request_id=request_id,
#                 )
#             )
        
#         # ç­‰å¾…æ‰€æœ‰è¯·æ±‚å®Œæˆ
#         raw_results = []
#         for output in outputs:
#             async for result in output:
#                 raw_results.append(result)
#                 break  # åªå–ç¬¬ä¸€ä¸ªç»“æœ
#         print(f"ğŸ§ reranker raw results: {len(raw_results)} items")
        
#         # è§£æç»“æœå¹¶æ ¼å¼åŒ–è¾“å‡º
#         processed_results = []
#         for i, result in enumerate(raw_results):
#             # æå–tokenå’Œlogprobä¿¡æ¯
#             output_text = result.outputs[0].text  # "yes" æˆ– "no"
#             logprobs = result.outputs[0].logprobs[0] if result.outputs[0].logprobs else {}
            
#             # è·å–yeså’Œnoçš„æ¦‚ç‡
#             yes_logprob = logprobs.get(self.true_token, None)
#             no_logprob = logprobs.get(self.false_token, None)
            
#             true_score = math.exp(yes_logprob.logprob) if yes_logprob else 0
#             false_score = math.exp(no_logprob.logprob) if no_logprob else 0
#             score = true_score / (true_score + false_score) if (true_score + false_score) > 0 else 0.5
            
#             processed_results.append({
#                 "index": i,
#                 "query": text_1[i] if i < len(text_1) else text_1[0] if text_1 else "",
#                 "document": text_2[i] if i < len(text_2) else text_2[0] if text_2 else "",
#                 "relevant": output_text == "yes",
#                 #"confidence": {
#                 #    "answer": output_text,
#                     # "yes_logprob": yes_logprob.logprob if yes_logprob else None,
#                     # "no_logprob": no_logprob.logprob if no_logprob else Noneï¼Œ
                    
#                 #}
#                 "score": score
#             })
        
#         return {
#             "model": "Qwen3-Reranker-0.6B",
#             "results": processed_results,
#             "total_pairs": len(processed_results)
#         }